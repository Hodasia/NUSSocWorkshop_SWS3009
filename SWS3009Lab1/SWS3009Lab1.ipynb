{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9e22a1",
   "metadata": {},
   "source": [
    "# SWS3009 Lab 1 - Generative LSTMs\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "(Source for this lab: https://medium.datadriveninvestor.com/story-generator-using-keras-step-by-step-lstm-in-rnn-32af3b467192)\n",
    "\n",
    "In this lab we are going to look at Long Short Term Memories (LSTMs). In the lecture we saw how LSTMs can be applied to regression problems, and in this lab we look at how to use LSTMs as generators to produce stories.\n",
    "\n",
    "<b>Important warning: Don't expect the stories to be of any usable quality. It will barely be readable. This is because we are using a very small dataset and a relatively simple network</b>\n",
    "\n",
    "## 2. Submission Instructions\n",
    "\n",
    "There is an Answer Book called Lab1AnsBk enclosed with this lab's ZIP file. \n",
    "\n",
    "Ensure that you have filled the names of BOTH team members in the answer book. If your name does not appear in any report you do not get any marks. \n",
    "\n",
    "<b>Fill in all answers in the answer book. Do not fill them inside here</b>\n",
    "\n",
    "Both Deep Learning members of the team should attempt this lab together. However each pair of students should submit only ONE copy of this report.\n",
    "\n",
    "Submission is to Canvas, in <b>PDF format ONLY</b>.\n",
    "\n",
    "This lab is worth 3 marks.\n",
    "\n",
    "Submission must be in English. Submissions in any other language will not be marked.\n",
    "\n",
    "Marking Scheme:\n",
    "\n",
    "0: No submission / Empty submission / Submission not in English<br>\n",
    "1: Poor submission<br>\n",
    "2: Average submission<br>\n",
    "3: Good submission<br>\n",
    "\n",
    "Before we begin ensure that you've installed all the libraries that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a259a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow transformers xformers datasets numpy sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948db396",
   "metadata": {},
   "source": [
    "## 3. Story Generation with LSTM\n",
    "\n",
    "We start first by looking at how to generate stories using an LSTM.  To do so we need to do two important things with the text:\n",
    "\n",
    "1. We need to tokenize the text, converting the words into integers.\n",
    "2. We need to use an embedding layer before feeding the words to the LSTM.\n",
    "\n",
    "### <u>Question 1</u>\n",
    "\n",
    "Using Google or otherwise, explain i) what an embedding layer does and ii) why we cannot just feed the integers from the tokenizer direct to the LSTM.\n",
    "\n",
    "<b>Fill your answers in the answer book</b>\n",
    "\n",
    "Let's begin by creating the dataset. When you unzipped the file containing this lab, it has created for you a text corpus in the sherlock directory, containing several Sherlock Holmes novels in a training directory and a testing directory. \n",
    "\n",
    "\n",
    "### 3.1 Loading the Dataset\n",
    "\n",
    "Keras has its own dataset manipulation libraries, but the one provided by Hugging Face is much more powerful and we will use it. We do the following:\n",
    "\n",
    "1. Gather all the files in the training and testing directories.\n",
    "2. Use load_dataset to load up all the texts.\n",
    "3. Remove all sentences that are too short.\n",
    "3. Create a special function to convert all the text to lowercase.\n",
    "4. Tokenize the dataset, converting all the words to integers.\n",
    "5. Combine the tokens into a single long vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24633b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Search for files in a directory matching a pattern.\n",
    "import glob\n",
    "\n",
    "# Gather all the files together\n",
    "traindir = \"sherlock/Train\"\n",
    "testdir = \"sherlock/Test\"\n",
    "\n",
    "# Get all the training and testing filenames\n",
    "train_files = [file for file in glob.glob(traindir + \"/*.txt\")]\n",
    "test_files = [file for file in glob.glob(testdir + \"/*.txt\")]\n",
    "\n",
    "# load_dataset needs a dictionary to tell it where the training and test files are\n",
    "data_files = {\"train\": train_files, \"test\":test_files}\n",
    "\n",
    "# Now load the dataset. We must also tell load_dataset that \n",
    "# these are text files\n",
    "dataset = load_dataset(\"text\", data_files = data_files)\n",
    "\n",
    "# Print out the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf5212",
   "metadata": {},
   "source": [
    "After running we can see that our training dataset consists of 19488 rows of text. We can see the first 10 lines  by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95573818",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307d748",
   "metadata": {},
   "source": [
    "Notice that many lines are blank or contain very few characters. Since sentences of 5 characters or less are unlikely to be meaningful, we will get rid of them. We will also apply a transform to convert all characters to lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56168110",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 5 # Minimum number of characters in a line\n",
    "\n",
    "# Remove lines with fewer than five characters\n",
    "dataset = dataset.filter(lambda example: len(example[\"text\"]) >=min_len)\n",
    "\n",
    "# This function is called by the dataset map method to convert\n",
    "# all the text to lowercase\n",
    "def tolower(example):\n",
    "    return {\"text\":example[\"text\"].lower()}\n",
    "\n",
    "# Convert all text using map\n",
    "dataset = dataset.map(tolower)\n",
    "\n",
    "# Now let's see what our dataset looks like\n",
    "dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62361e00",
   "metadata": {},
   "source": [
    "As we can see, the data is much neater now. Our next step is to use a tokenizer to convert the sentences into integer vectors. Instead of the standard Keras tokenizer, we will use the one from Hugging Face which is much more powerful and convenient to use, particular when we start using transformers in the next lab.\n",
    "\n",
    "The version we are using is pretrained on the OpenAI GPT2 tokenizer. For LSTMs we do not need to pad or truncate lines to fixed lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ccb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Import the OpenAI GPT2 tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Specify the padding token to be the end-of-sentence token\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Vocabulary size: \", len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfa587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now tokenize a statement\n",
    "test_stat = \"Elementary, my dear Watson\"\n",
    "tokens = tokenizer(test_stat, padding=False, truncation=False, return_length=True)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e6dfe",
   "metadata": {},
   "source": [
    "As we can see from above tokenizer turns our sentence into a series of integers. Now let's tokenize the entire corpus, once again using the map function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't bother returning the lengths\n",
    "def tokenize(example):\n",
    "    retlist = []\n",
    "    output = tokenizer(example[\"text\"], padding=False, truncation=False,\n",
    "                      return_overflowing_tokens=True)   \n",
    "    \n",
    "    for token in output[\"input_ids\"]:\n",
    "        retlist.append(token)\n",
    "\n",
    "    return {\"input_ids\":retlist}\n",
    "\n",
    "# Remove the existing columns so that we are left only with an input_ids column\n",
    "token_dataset = dataset.map(tokenize, batched=True, \n",
    "                            remove_columns=dataset['train'].column_names)\n",
    "\n",
    "token_dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8602b",
   "metadata": {},
   "source": [
    "If we look at what has happened, we see that the entire dataset has been turned into tokens - integers that represent words. Since we specified that we should not pad or truncate the lines, every line has a different length. This is OK for LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6835baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for toks in token_dataset['train'][:10]['input_ids']:\n",
    "    print(len(toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aede93",
   "metadata": {},
   "source": [
    "### 3.2 Handling Large Datasets\n",
    "\n",
    "Now we have our dataset nicely tokenized. For transformers, this is enough. Unfortunately for LSTMs, we need to generate sequences and teach the LSTM how to predict the next word based on the past few words.\n",
    "\n",
    "We begin by compiling all the tokens in the sentence into a giant array, then chop up the array into slices of 5 words for the LSTM to predict the 6th using the Keras TimeseriesGenerator class.\n",
    "\n",
    "### <u>Question 2</u>\n",
    "\n",
    "Explain why we don't need to chop up our tokens into groups of 5 tokens to predict the 6th for transformers, but must do so for LSTMs.\n",
    "\n",
    "<b>Fill your answers in the answer book</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2887aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltokens = []\n",
    "for sentences in token_dataset[\"train\"]:\n",
    "    alltokens.extend(sentences['input_ids'])\n",
    "    \n",
    "print(\"Total number of tokens: \", len(alltokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394e26e",
   "metadata": {},
   "source": [
    "As you can see we have quite a lot of tokens. One important point is that we are unlikely to be able to fit all our training sequences into memory, so we will instead create a generator. Fortunately Keras provides us with the TimeseriesGenerator class, which will chop up our samples in fixed sizes, and produce the next token to be predicted.\n",
    "\n",
    "We do this for both our training and testing data.\n",
    "\n",
    "Note however that we need to convert our next token to a one-hot vector. We also adjust our token vectors to be divisible by the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5481afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "batch_size = 32\n",
    "lookback = 5\n",
    "\n",
    "# Ensure that the number of tokens is divisible by batch_size\n",
    "old_len = len(alltokens)\n",
    "new_len = (int) (old_len / batch_size) * batch_size\n",
    "\n",
    "print(\"Old length: \", old_len, \"New length: \", new_len)\n",
    "\n",
    "alltokens = alltokens[:new_len]\n",
    "outputs = to_categorical(alltokens, vocab_size)\n",
    "seqgen = TimeseriesGenerator(alltokens, outputs, length=lookback, batch_size=batch_size)\n",
    "\n",
    "# We need to do the same for the testing data\n",
    "alltokens_test = []\n",
    "\n",
    "for sentences in token_dataset[\"test\"]:\n",
    "    alltokens_test.extend(sentences[\"input_ids\"])\n",
    "\n",
    "print(\"Total number of testing tokens: \", len(alltokens_test))\n",
    "old_len = len(alltokens_test)\n",
    "new_len = (int) (old_len / batch_size) * batch_size\n",
    "print(\"Old length: \", old_len, \" New length: \", new_len)\n",
    "alltokens_test = alltokens_test[:new_len]\n",
    "outputs_test = to_categorical(alltokens_test, vocab_size)\n",
    "seqgen_test = TimeseriesGenerator(alltokens_test, outputs_test, length=lookback,\n",
    "                                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9409231",
   "metadata": {},
   "source": [
    "### 3.3 Building and Training the Network\n",
    "\n",
    "Now that we have our datasets properly formatted and have created our training and testing generators, let's proceed to build our model, or load it from disk if one is already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf1d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import os\n",
    "\n",
    "filename=\"sherlock.h5\"\n",
    "# If the model file exists, we reload from there instead of creating a new model\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing model from \", filename)\n",
    "    model = load_model(filename)\n",
    "else:\n",
    "    print(\"Creating new model.\")\n",
    "    # Create our model\n",
    "    n_units = 256 # Hidden layer size for our LSTM\n",
    "    embedding_size=100 # Size of embedding layer vectors\n",
    "\n",
    "    text_in = Input(shape=(None, ))\n",
    "    embedding = Embedding(vocab_size, embedding_size)(text_in)\n",
    "    lstm = LSTM(n_units)(embedding)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lstm)\n",
    "\n",
    "    model = Model(inputs = text_in, outputs = outputs)\n",
    "\n",
    "    # Set a slower learning rate\n",
    "    learning_rate = 0.001\n",
    "    opti = RMSprop(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=opti)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb0b41",
   "metadata": {},
   "source": [
    "### <u>Question 3</u>\n",
    "\n",
    "i. In our network we have used a one-hot approach; our network will have over 50,000 outputs, where one of them will be set to \"1\" and the rest to \"0\" when training. Why can't we just have one output, where the target value is the index of the next word?\n",
    "\n",
    "ii. Why do we use softmax and categorical cross entropy for the activation function and loss function?\n",
    "\n",
    "<b>Fill your answers in the answer book</b>\n",
    "\n",
    "This is great! We can now begin training our LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "# This will take a LONG time. \n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Callback to save the model\n",
    "newlen_train = len(alltokens)\n",
    "save_model = ModelCheckpoint(filename)\n",
    "\n",
    "# Early stopping callback to prevent overfitting (may cause underfitting)\n",
    "# Stop if change between validation losses is under 0.01 twice.\n",
    "earlystop = EarlyStopping(min_delta = 0.01, patience = 4)\n",
    "\n",
    "steps_per_epoch = (int)(newlen_train / batch_size)\n",
    "\n",
    "print(\"Expected number of training vectors: \", steps_per_epoch)\n",
    "model.fit(seqgen, epochs=epochs, steps_per_epoch = steps_per_epoch, batch_size=batch_size,\n",
    "          validation_data = seqgen_test, callbacks = [save_model, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ec9d5",
   "metadata": {},
   "source": [
    "### 3.4 Text Generation\n",
    "\n",
    "Now comes the fun part! We will now use our model to create stories. These are the steps we need to take:\n",
    "\n",
    "1. Create a prompt. This is usually the first few words of the starting sentence of our story.\n",
    "2. Tokenize the prompt.\n",
    "3. Feed it to the network.\n",
    "4. Use a probability model to choose which output we want, based on the current series of words. I.e. we choose $nextword = argmax_{w_i}P(w_i | w_{i-1})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temp(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_text(seed_text, next_words, model, lookback, temp):\n",
    "    output_text = seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        \n",
    "        token_list = tokenizer.encode(seed_text, return_tensors=\"tf\")\n",
    "        token_list = token_list[0][-lookback:]\n",
    "#        print(token_list)\n",
    "#        print(\"TOKEN LIST LEN: \", len(token_list))\n",
    "        token_list = np.reshape(token_list, (1, lookback))\n",
    "        \n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        y_class = sample_with_temp(probs, temperature = temp)\n",
    "        \n",
    "        if y_class != 220:\n",
    "            output_words = tokenizer.convert_ids_to_tokens([y_class], \n",
    "                                                           skip_special_tokens=True)\n",
    "        else:\n",
    "            output_words=\"\"\n",
    "            \n",
    "        for output_word in output_words:\n",
    "            if output_word[0] == 'Ġ':\n",
    "                output_word = output_word[1:]\n",
    "            output_text += output_word + \" \"\n",
    "            seed_text += output_word + \" \"\n",
    "            \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75908a65",
   "metadata": {},
   "source": [
    "Now let's generate some text!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=3\n",
    "seed_text = \"elementary my dear watson, \"\n",
    "genwords = 1000\n",
    "\n",
    "print(\"Temperature = \", temp)\n",
    "out_text = generate_text(seed_text, genwords, model, lookback, temp=temp)\n",
    "\n",
    "print(\"\\nGenerated text: \")\n",
    "print(out_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfde90f",
   "metadata": {},
   "source": [
    "## 4 Conclusion\n",
    "\n",
    "We have just seen how to use LSTMs to generate texts based on a corpus and some seed text. The idea is for the LSTM to learn to predict the next word to be generated based on a current set of words.\n",
    "\n",
    "We made use of a TimeseriesGenerator to produce the values on-the-fly as the dataset is too large to be fully loaded into memory.\n",
    "\n",
    "In the next lab we will look at how to use build transformers and use them to generate texts. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e1382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
